{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqHQyhnrD9nw"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# ================================================================\n",
        "# 1) Judge 모델 로드\n",
        "# ================================================================\n",
        "JUDGE_MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "\n",
        "judge_model, judge_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=JUDGE_MODEL_NAME,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(judge_model)\n",
        "\n",
        "# ================================================================\n",
        "# 2) Judge Prompt (출력 형식 고정 / 평가 + 피드백만)\n",
        "# ================================================================\n",
        "def build_judge_prompt(user_prompt: str) -> str:\n",
        "    return f\"\"\"\n",
        "너는 사용자가 작성한 프롬프트의 품질을 평가하는 전문가이다.\n",
        "답변 내용이 아니라 프롬프트 자체만 평가한다.\n",
        "\n",
        "출력은 반드시 JSON 객체 1개만 작성한다.\n",
        "JSON 앞뒤로 어떤 문자도 출력하지 않는다.\n",
        "\n",
        "================================================================\n",
        "# 평가 기준 (0~100)\n",
        "================================================================\n",
        "1) 명확성\n",
        "- 목표가 분명하지 않으면 감점한다.\n",
        "- 여러 해석이 가능한 표현을 감점한다.\n",
        "\n",
        "2) 구체성\n",
        "- 누가 / 무엇을 / 언제 / 어디서 / 왜 / 어떻게 중\n",
        "  빠진 요소마다 감점한다.\n",
        "- 출력 형태나 분량 지시가 없으면 감점한다.\n",
        "- 제약 조건이 없으면 감점한다.\n",
        "\n",
        "3) 구성\n",
        "- 지시문, 맥락, 제약 조건, 출력 형태가\n",
        "  체계적으로 구성되지 않으면 감점한다.\n",
        "\n",
        "4) 언어 품질\n",
        "- 문법적으로 어색한 문장을 감점한다.\n",
        "- 자연스럽지 않은 표현을 감점한다.\n",
        "\n",
        "5) 일관성\n",
        "- 서로 충돌하는 요구가 있으면 크게 감점한다.\n",
        "- 하나의 의미로 해석되지 않으면 감점한다.\n",
        "\n",
        "================================================================\n",
        "# 한국어 전용 규칙\n",
        "================================================================\n",
        "- 모든 코멘트와 피드백은 100% 자연스러운 한국어로 작성한다.\n",
        "- 영어 단어, 약어, 기호 표현을 절대 사용하지 않는다.\n",
        "- \"일부\", \"약간\", \"조금\", \"다소\", \"애매함\", \"부분적으로\" 같은 표현을 쓰지 않는다.\n",
        "- 모든 지적은 구체적이고 실행 가능해야 한다.\n",
        "\n",
        "================================================================\n",
        "# 원문 의도 보존 규칙\n",
        "================================================================\n",
        "- 원문 프롬프트의 핵심 의도를 절대로 변경하거나 확장하지 마라.\n",
        "- 새로운 목적이나 작업을 제안하지 마라.\n",
        "- 대상 독자나 사용 상황을 임의로 추가하지 마라.\n",
        "- 원문에 없는 맥락을 가정하지 마라.\n",
        "\n",
        "개선 피드백은 다음 범위 안에서만 작성한다:\n",
        "- 표현을 더 명확하게 만드는 방향\n",
        "- 빠진 정보를 보완하도록 안내\n",
        "- 출력 형태나 제약 조건을 명확히 하도록 안내\n",
        "\n",
        "================================================================\n",
        "# overall_score 계산 규칙\n",
        "================================================================\n",
        "overall_score = round(\n",
        "    (clarity_score + specificity_score + structure_score +\n",
        "     language_score + consistency_score) / 5\n",
        ")\n",
        "\n",
        "================================================================\n",
        "# JSON 출력 형식 (반드시 이 구조만 사용)\n",
        "================================================================\n",
        "{{\n",
        "  \"overall_score\": <0-100>,\n",
        "  \"clarity_score\": <0-100>,\n",
        "  \"specificity_score\": <0-100>,\n",
        "  \"structure_score\": <0-100>,\n",
        "  \"language_score\": <0-100>,\n",
        "  \"consistency_score\": <0-100>,\n",
        "  \"clarity_comment\": \"<명확성 평가 및 개선 조언>\",\n",
        "  \"specificity_comment\": \"<구체성 평가 및 개선 조언>\",\n",
        "  \"structure_comment\": \"<구성 평가 및 개선 조언>\",\n",
        "  \"language_comment\": \"<언어 품질 평가 및 개선 조언>\",\n",
        "  \"consistency_comment\": \"<일관성 평가 및 개선 조언>\",\n",
        "  \"summary_feedback\": \"<전체 프롬프트를 개선하기 위한 핵심 조언>\"\n",
        "}}\n",
        "\n",
        "================================================================\n",
        "# 평가 대상 프롬프트\n",
        "================================================================\n",
        "{user_prompt}\n",
        "\"\"\".strip()\n",
        "\n",
        "# ================================================================\n",
        "# 3) 모델 generate 함수 (결정적)\n",
        "# ================================================================\n",
        "def generate(model, tokenizer, messages, max_new_tokens=512) -> str:\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            top_p=1.0,\n",
        "            top_k=50,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    text = tokenizer.decode(\n",
        "        output[0][input_ids.shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    ).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# ================================================================\n",
        "# 4) JSON 안전 파싱\n",
        "# ================================================================\n",
        "def safe_json_extract(text: str) -> dict:\n",
        "    start = text.find(\"{\")\n",
        "    end = text.rfind(\"}\")\n",
        "\n",
        "    if start == -1 or end == -1:\n",
        "        raise ValueError(\"JSON을 찾을 수 없습니다:\\n\" + text)\n",
        "\n",
        "    json_str = text[start:end + 1]\n",
        "\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        json_str = json_str.replace(\"\\n\", \"\")\n",
        "        json_str = json_str.replace(\",}\", \"}\")\n",
        "        return json.loads(json_str)\n",
        "\n",
        "# ================================================================\n",
        "# 5) 외부 호출용 평가 함수\n",
        "# ================================================================\n",
        "def evaluate_prompt_only(user_prompt: str) -> dict:\n",
        "    judge_prompt = build_judge_prompt(user_prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"너는 프롬프트 품질을 평가하는 결정적 평가자이다.\"},\n",
        "        {\"role\": \"user\", \"content\": judge_prompt},\n",
        "    ]\n",
        "\n",
        "    raw_output = generate(judge_model, judge_tokenizer, messages)\n",
        "    return safe_json_extract(raw_output)\n",
        "\n",
        "# ================================================================\n",
        "# 6) 테스트 실행\n",
        "# ================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    test_prompt = \"모델서빙하는 방ㅂㅂㅓㅂ을 잘 모륵겟어 설명해줘 \"\n",
        "    result = evaluate_prompt_only(test_prompt)\n",
        "    print(json.dumps(result, ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "id": "NQVL8rANEEBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}