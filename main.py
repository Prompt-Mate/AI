# ============================================================
# ğŸ”¥ PROMMATE AI SERVER (Rewrite + Judge í†µí•© / Transformers CPU)
# ============================================================
from fastapi import FastAPI
from pydantic import BaseModel
import torch
import json

from transformers import AutoModelForCausalLM, AutoTokenizer


# ============================================================
# 1) FastAPI APP
# ============================================================
app = FastAPI(
    title="PromMate Unified AI API",
    description="Rewrite + Judge Model Server (Transformers CPU)",
    version="1.0.0"
)


# ============================================================
# 2) ---------------- Judge Model ë¡œë“œ ------------------------
# ============================================================
JUDGE_MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"   # CPU ì‹¤í–‰ ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ë³€ê²½

print("Loading Judge Model...")
judge_tokenizer = AutoTokenizer.from_pretrained(JUDGE_MODEL_NAME)
judge_model = AutoModelForCausalLM.from_pretrained(
    JUDGE_MODEL_NAME,
    torch_dtype=torch.float32,
).to("cpu")
print("Judge Model Loaded")


# ============================================================
# Judge Prompt (ë„¤ê°€ ì‘ì„±í•œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€)
# ============================================================
def build_judge_prompt(user_prompt: str) -> str:
    return f"""
ë„ˆëŠ” ì‚¬ìš©ìê°€ ì‘ì„±í•œ í”„ë¡¬í”„íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì´ë‹¤.
ë‹µë³€ ë‚´ìš©ì´ ì•„ë‹ˆë¼ í”„ë¡¬í”„íŠ¸ ìì²´ë§Œ í‰ê°€í•œë‹¤.

ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON ê°ì²´ 1ê°œë§Œ ì‘ì„±í•œë‹¤.
JSON ì•ë’¤ë¡œ ì–´ë–¤ ë¬¸ìë„ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.

================================================================
# í‰ê°€ ê¸°ì¤€ (0~100)
================================================================
1) ëª…í™•ì„±
- ëª©í‘œê°€ ë¶„ëª…í•˜ì§€ ì•Šìœ¼ë©´ ê°ì í•œë‹¤.
- ì—¬ëŸ¬ í•´ì„ì´ ê°€ëŠ¥í•œ í‘œí˜„ì„ ê°ì í•œë‹¤.

2) êµ¬ì²´ì„±
- ëˆ„ê°€ / ë¬´ì—‡ì„ / ì–¸ì œ / ì–´ë””ì„œ / ì™œ / ì–´ë–»ê²Œ ì¤‘
  ë¹ ì§„ ìš”ì†Œë§ˆë‹¤ ê°ì í•œë‹¤.
- ì¶œë ¥ í˜•íƒœë‚˜ ë¶„ëŸ‰ ì§€ì‹œê°€ ì—†ìœ¼ë©´ ê°ì í•œë‹¤.
- ì œì•½ ì¡°ê±´ì´ ì—†ìœ¼ë©´ ê°ì í•œë‹¤.

3) êµ¬ì„±
- ì§€ì‹œë¬¸, ë§¥ë½, ì œì•½ ì¡°ê±´, ì¶œë ¥ í˜•íƒœê°€
  ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±ë˜ì§€ ì•Šìœ¼ë©´ ê°ì í•œë‹¤.

4) ì–¸ì–´ í’ˆì§ˆ
- ë¬¸ë²•ì ìœ¼ë¡œ ì–´ìƒ‰í•œ ë¬¸ì¥ì„ ê°ì í•œë‹¤.
- ìì—°ìŠ¤ëŸ½ì§€ ì•Šì€ í‘œí˜„ì„ ê°ì í•œë‹¤.

5) ì¼ê´€ì„±
- ì„œë¡œ ì¶©ëŒí•˜ëŠ” ìš”êµ¬ê°€ ìˆìœ¼ë©´ í¬ê²Œ ê°ì í•œë‹¤.
- í•˜ë‚˜ì˜ ì˜ë¯¸ë¡œ í•´ì„ë˜ì§€ ì•Šìœ¼ë©´ ê°ì í•œë‹¤.

================================================================
# í•œêµ­ì–´ ì „ìš© ê·œì¹™
================================================================
- ëª¨ë“  ì½”ë©˜íŠ¸ì™€ í”¼ë“œë°±ì€ 100% ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
- ì˜ì–´ ë‹¨ì–´, ì•½ì–´, ê¸°í˜¸ í‘œí˜„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
- "ì¼ë¶€", "ì•½ê°„", "ì¡°ê¸ˆ", "ë‹¤ì†Œ", "ì• ë§¤í•¨", "ë¶€ë¶„ì ìœ¼ë¡œ" ê°™ì€ í‘œí˜„ì„ ì“°ì§€ ì•ŠëŠ”ë‹¤.
- ëª¨ë“  ì§€ì ì€ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•œë‹¤.

================================================================
# ì›ë¬¸ ì˜ë„ ë³´ì¡´ ê·œì¹™
================================================================
- ì›ë¬¸ í”„ë¡¬í”„íŠ¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ì ˆëŒ€ë¡œ ë³€ê²½í•˜ê±°ë‚˜ í™•ì¥í•˜ì§€ ë§ˆë¼.
- ìƒˆë¡œìš´ ëª©ì ì´ë‚˜ ì‘ì—…ì„ ì œì•ˆí•˜ì§€ ë§ˆë¼.
- ëŒ€ìƒ ë…ìë‚˜ ì‚¬ìš© ìƒí™©ì„ ì„ì˜ë¡œ ì¶”ê°€í•˜ì§€ ë§ˆë¼.
- ì›ë¬¸ì— ì—†ëŠ” ë§¥ë½ì„ ê°€ì •í•˜ì§€ ë§ˆë¼.

ê°œì„  í”¼ë“œë°±ì€ ë‹¤ìŒ ë²”ìœ„ ì•ˆì—ì„œë§Œ ì‘ì„±í•œë‹¤:
- í‘œí˜„ì„ ë” ëª…í™•í•˜ê²Œ ë§Œë“œëŠ” ë°©í–¥
- ë¹ ì§„ ì •ë³´ë¥¼ ë³´ì™„í•˜ë„ë¡ ì•ˆë‚´
- ì¶œë ¥ í˜•íƒœë‚˜ ì œì•½ ì¡°ê±´ì„ ëª…í™•íˆ í•˜ë„ë¡ ì•ˆë‚´

================================================================
# overall_score ê³„ì‚° ê·œì¹™
================================================================
overall_score = round(
    (clarity_score + specificity_score + structure_score +
     language_score + consistency_score) / 5
)

================================================================
# JSON ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë§Œ ì‚¬ìš©)
================================================================
{{
  "overall_score": <0-100>,
  "clarity_score": <0-100>,
  "specificity_score": <0-100>,
  "structure_score": <0-100>,
  "language_score": <0-100>,
  "consistency_score": <0-100>,
  "clarity_comment": "<ëª…í™•ì„± í‰ê°€ ë° ê°œì„  ì¡°ì–¸>",
  "specificity_comment": "<êµ¬ì²´ì„± í‰ê°€ ë° ê°œì„  ì¡°ì–¸>",
  "structure_comment": "<êµ¬ì„± í‰ê°€ ë° ê°œì„  ì¡°ì–¸>",
  "language_comment": "<ì–¸ì–´ í’ˆì§ˆ í‰ê°€ ë° ê°œì„  ì¡°ì–¸>",
  "consistency_comment": "<ì¼ê´€ì„± í‰ê°€ ë° ê°œì„  ì¡°ì–¸>",
  "summary_feedback": "<ì „ì²´ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ í•µì‹¬ ì¡°ì–¸>"
}}

================================================================
# í‰ê°€ ëŒ€ìƒ í”„ë¡¬í”„íŠ¸
================================================================
{user_prompt}
""".strip()



# ============================================================
#  Judge Inference (ë„¤ ë°©ì‹ ê·¸ëŒ€ë¡œ ìœ ì§€, Unsloth â†’ Transformers ë°©ì‹ë§Œ ë³€ê²½)
# ============================================================
def generate(model, tokenizer, messages, max_new_tokens=512) -> str:
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )

    inputs = tokenizer(text, return_tensors="pt").to("cpu")

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.0,
            do_sample=False,
            top_p=1.0,
            top_k=50,
        )

    return tokenizer.decode(output[0], skip_special_tokens=True)


def safe_json_extract(text: str) -> dict:
    start, end = text.find("{"), text.rfind("}")
    json_str = text[start:end + 1]
    return json.loads(json_str)


def evaluate_prompt_only(user_prompt: str) -> dict:
    judge_prompt = build_judge_prompt(user_prompt)
    messages = [
        {"role": "system", "content": "ë„ˆëŠ” í”„ë¡¬í”„íŠ¸ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ê²°ì •ì  í‰ê°€ìë‹¤."},
        {"role": "user", "content": judge_prompt},
    ]
    raw = generate(judge_model, judge_tokenizer, messages)
    return safe_json_extract(raw)



# ============================================================
# 3) Rewrite Model (transformersë¡œ ê·¸ëŒ€ë¡œ ë³€í™˜)
# ============================================================
BASE_MODEL = "meta-llama/Llama-3.2-3B-Instruct"  # CPUìš©

print("Loading Rewrite Model...")
rewrite_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
rewrite_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float32,
).to("cpu")
print("Rewrite Model Loaded")


def rewrite_final_strict(user_prompt: str, domain="ê¸°íƒ€") -> str:
    system_prompt = (
        "ë‹¹ì‹ ì€ ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ë¦¬ë¼ì´íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n\n"
        "ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ìŒ ê¸°ì¤€ì„ ëª¨ë‘ ì¶©ì¡±í•˜ëŠ” "
        "í•˜ë‚˜ì˜ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¡œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "- ê¸°ì¤€ ë‚˜ì—´ ê¸ˆì§€\n"
        "- ì‹¤ì œ ë‹µë³€ ìƒì„± ê¸ˆì§€\n"
        "- í”„ë¡¬í”„íŠ¸ 1ê°œë§Œ ì¶œë ¥\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    text = rewrite_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = rewrite_tokenizer(text, return_tensors="pt").to("cpu")

    output = rewrite_model.generate(
        **inputs,
        max_new_tokens=300,
        temperature=0.4,
        top_p=0.9,
        do_sample=True,
    )

    return rewrite_tokenizer.decode(output[0], skip_special_tokens=True)



# ============================================================
# 4) API Request/Response Schema (ì›ë³¸ ê·¸ëŒ€ë¡œ)
# ============================================================
class PromptReq(BaseModel):
    prompt: str

class RewriteRes(BaseModel):
    rewrite_final: str

class JudgeRes(BaseModel):
    overall_score: int
    clarity_score: int
    specificity_score: int
    structure_score: int
    language_score: int
    consistency_score: int
    clarity_comment: str
    specificity_comment: str
    structure_comment: str
    language_comment: str
    consistency_comment: str
    summary_feedback: str



# ============================================================
# 5) Endpoints (ë³€ê²½ ì—†ìŒ)
# ============================================================
@app.post("/rewrite", response_model=RewriteRes)
def rewrite_api(req: PromptReq):
    return {"rewrite_final": rewrite_final_strict(req.prompt)}

@app.post("/judge", response_model=JudgeRes)
def judge_api(req: PromptReq):
    return evaluate_prompt_only(req.prompt)

@app.get("/health")
def health():
    return {"status": "ok"}
